{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling \n",
    "\n",
    "Un **modelo** es una representación simplificada de la realidad creada para servir una propuesta. Las afirmaciones, la selección de características importantes, las restricciones y la trazabilidad definen un modelo. Un ejemplo de un modelo es un mapa que nos posibilita posicionar lugares de interes, calcular distancias entre ellos, o encontrar relaciones topológicas.\n",
    "\n",
    "<img src=\"images/mapprojection.png\" width=\"70%\"/>\n",
    "\n",
    "\n",
    "En ciencia de datos podemos tener modelos predictivos o modelos descriptivos. Los primeros forman un conjunto de formulas para estimar valores desconocidos. En el segundo caso, sirven para encontrar indicios subyacentes del proceso. Los datos del modelo representan hechos o evidencias de la realidad o de otro modelo. Siguiendo el ejemplo del mapa, represetan los bordes o contornos del territorio representado.\n",
    "\n",
    "<img src=\"images/modeloDatos.png\" width=\"70%\"/>\n",
    "<img src=\"images/people.png\" width=\"70%\"/>\n",
    "\n",
    "> Fuente de ambas figuras [2]\n",
    "\n",
    "\n",
    "## El proceso de aprendizaje\n",
    "\n",
    "El proceso de aprendizaje automático suele constar de una serie de pasos. Por lo general, estos son:\n",
    "- Preparación de datos. Carga y limpieza de datos.\n",
    "- Selección de atributos o métricas adecuadas.\n",
    "- Selección de la técnica a aplicar.\n",
    "- Ajuste de los hiperparámetros.\n",
    "- Evaluación del modelo. \n",
    "\n",
    "\n",
    "## Selección de Atributos \n",
    "Dado un conjunto amplio de atributos que caracterizan los eventos a representar, **el problema es elegir aquellos que puedan contribuir al modelo de aprendizaje**.\n",
    "\n",
    "En el ejemplo de las personas, los argumentos que podríamos tener son:\n",
    "- Forma de la cabeza: cuadrada o circular.\n",
    "- Forma del cuerpo: rectangular o óvalo.\n",
    "- Color del cuerpo: negro o blanco. \n",
    "  \n",
    "Si tuvieramos que segmentar dicha población deberíamos de plantear que atributos de estas personas nos permitiran identificar, por ejemplo, si cancelerán su compra (*write-offs* a *yes/no*). Este atributo es la variable objetivo: *target variable*. El diseño del modelo ha de contener una homogeneidad con respecto a esta variable. No puede existir un individuo que esté en ambos targets. Estaríamos tratando otro problema. \n",
    "\n",
    "En este problema de segmentación, el objetivo sería encontrar una o varias variables que defininan **puramente** un grupo. Rara vez suele pasar. Por ejemplo, el atributo head-shape no lo es.  Si tan solo eligieramos el atributo body-color a negro, no tendríamos una representatividad del resto de la población. E incluso, hay que tratar con valores númericos de atributos continuos o discretos.\n",
    "\n",
    "En el campo de la Teoria de la Información, Shannon en 1948 definió dos conceptos importantes **information gain (IG)** y **entropy**.\n",
    "\n",
    "La entropia es un indicador sobre el \"desorden\":<br/>\n",
    "$ H = - \\sum^{W}_{i=1}p_i * log(p_i)$<br/>\n",
    "\n",
    "$p(write-off)= 7/12= 0.583$ <br/>\n",
    "$p(non-write-off)= 5/12 = 0.416$<br/>\n",
    "\n",
    "$ H = - [ 7/12 * log(7/12) + 5/12 * log(5/12)] = 0.6791 $ \n",
    "\n",
    "La ganancia de información de un atributo con respecto a la variable objetivo representa cuanto aporta dicha variable para mejorar (reducir) la entropía debido al aumento de información. \n",
    "\n",
    "$ IG(parent, children) = entropy(parent) - [ p(c_1)*entropy(c_1)+ p(c_2)*entropy(c_2)+...]$ \n",
    "\n",
    "<br/>\n",
    "Veamos un caso: <br/>\n",
    "\n",
    "<img src=\"images/IGcase.png\" width=\"60%\"/>\n",
    "\n",
    "> Fuente de la imagen [2] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La entropia de toda la población con respecto a $\\bullet$  y $\\star$ es: <br/>\n",
    "\n",
    "$ entropy(parent) = - [ p(\\bullet)*log(\\bullet) +  p(\\star) *log(\\star)] $<br/> \n",
    "$ entropy(parent) = - [ 0.53*-0.9  + 0.47*-1.1] = 0.99 $  \n",
    "Nota: un valor muy alto, muy impuro.\n",
    "\n",
    "\n",
    "Si seleccionamos el atributo de *balance* con un criterio de $50k$ dispondremos de dos poblaciones nuevas con entropia diferentes.\n",
    "\n",
    "$ entropy(balance < 50k) = - [ p(\\bullet)*log(\\bullet) +  p(\\star) *log(\\star)] $<br/> \n",
    "$ entropy(balance < 50k) = - [ 0.92 * -0.12 + 0.08*-3.7] = 0.39 $<br/> \n",
    "\n",
    "$ entropy(balance \\geq 50k) = - [ p(\\bullet)*log(\\bullet) +  p(\\star) *log(\\star)] $<br/> \n",
    "$ entropy(balance \\geq 50k) = - [ 0.24 * -2.1 + 0.76*-0.39] = 0.79 $<br/> \n",
    "\n",
    "\n",
    "$IG =  entropy(parent) - [ p(balance < 50k)*entropy(balance < 50k)+ p(balance \\geq 50k)*entropy(balance \\geq 50k) ]$ <br/>\n",
    "$IG =  entropy(parent) - [ 13/30 *entropy(balance < 50k)+ 17/30*entropy(balance \\geq 50k) ]$ <br/>\n",
    "\n",
    "$ IG = 0.99 - [0.43 * 0.39 + 0.57 * 0.79] = 0.37 $\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Ganancia y Entropía de Información con Python\n",
    "\n",
    "Vamos a utilizar el siguiente catálogo:\n",
    "\n",
    "- Mushroom Data Set http://archive.ics.uci.edu/ml/datasets/Mushroom\n",
    "    - data/mushrooms.csv\n",
    "    \n",
    "\n",
    "\n",
    "¿Qué librerías de ML se usan habitualmente en Python ?\n",
    "\n",
    "- Scikit-learn: https://scikit-learn.org/stable/index.html\n",
    "\n",
    "Instalamos dependencias:\n",
    "```bash\n",
    "pip install pandas\n",
    "pip install numpy\n",
    "pip install matplotlib\n",
    "pip install seaborn\n",
    "pip install sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 23)\n",
      "Index(['class', 'cap-shape', 'cap-surface', 'cap-color', 'bruises', 'odor',\n",
      "       'gill-attachment', 'gill-spacing', 'gill-size', 'gill-color',\n",
      "       'stalk-shape', 'stalk-root', 'stalk-surface-above-ring',\n",
      "       'stalk-surface-below-ring', 'stalk-color-above-ring',\n",
      "       'stalk-color-below-ring', 'veil-type', 'veil-color', 'ring-number',\n",
      "       'ring-type', 'spore-print-color', 'population', 'habitat'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/mushrooms.csv\")\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log, e\n",
    "import numpy as np \n",
    "\n",
    "def entropy(serie):\n",
    "  value,counts = np.unique(serie, return_counts=True)\n",
    "  norm_counts = counts / counts.sum()\n",
    "  return -np.sum(norm_counts * np.log(norm_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attribute: class  0.6925010959051001\n",
      "Attribute: cap-surface  1.0920439563177977\n",
      "Attribute: gill-size  0.6184649299084096\n"
     ]
    }
   ],
   "source": [
    "print(\"Attribute: class \" , entropy(df[\"class\"]))\n",
    "print(\"Attribute: cap-surface \" , entropy(df[\"cap-surface\"]))\n",
    "print(\"Attribute: gill-size \" , entropy(df[\"gill-size\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde más fácil resulta calcular la entropia es en árboles de decisión, por lo que se puede ver en la *firma/signature* de algunas funciones de scikit.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "```text\n",
    "criterion{“gini”, “entropy”, “log_loss”}, default=”gini”\n",
    "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “log_loss” and “entropy” both for the Shannon information gain, see Mathematical formulation.\n",
    "```\n",
    "\n",
    "```python\n",
    "model = sklearn.tree.DecisionTreeClassifier(criterion='entropy')\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "> **Nota** <br/>\n",
    "> Profundizaremos este punto a medida que vayamos introduciendo algoritmos de ML con Python\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un ejemplo de ML algorithm\n",
    "\n",
    "Veremos los principales métodos de Scikit-Learn library que nos proporciona la estructura basica de un algoritmo de ML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8124, 23)\n",
      "0    s\n",
      "1    n\n",
      "2    n\n",
      "3    s\n",
      "4    a\n",
      "Name: population, dtype: object\n",
      "['a' 'c' 'n' 's' 'v' 'y'] [ 384  340  400 1248 4040 1712]\n",
      "x_train: (6499, 22)\n",
      "x_test:  (1625, 22)\n",
      "y_train:  (6499,)\n"
     ]
    }
   ],
   "source": [
    "# Algoritmo: clasificación de hongos según su población\n",
    "# Método supervisado\n",
    "\n",
    "# 1º Datos\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/mushrooms.csv\")\n",
    "print(df.shape)\n",
    "\n",
    "# 2º Target value:\n",
    "# 21. population: abundant=a,clustered=c,numerous=n, scattered=s,several=v,solitary=y\n",
    "print(df.population.head()) \n",
    "value,counts = np.unique(df.population, return_counts=True)\n",
    "print(value,counts)\n",
    "\n",
    "df_y = df.population.copy()\n",
    "df_x = df.drop(labels=[\"population\"],axis=1)\n",
    "\n",
    "# 3º Feature selection: Todos.\n",
    "\n",
    "# 4º Data splitting. Crear modelo de testing.\n",
    "# Datos de entrenamiento (train) y datos de comprobación (test)\n",
    "# https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test,  y_train, y_test = train_test_split(df_x,df_y, test_size=0.2, random_state = 0)\n",
    "print(\"x_train:\", x_train.shape)\n",
    "print(\"x_test: \",x_test.shape)\n",
    "print(\"y_train: \",y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'e'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/isaac/Projects/TxADM_notebooks/docs/notebooks/Part2/03_MachineLearningNotes/02_PredictiveModelling.ipynb Cell 9\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isaac/Projects/TxADM_notebooks/docs/notebooks/Part2/03_MachineLearningNotes/02_PredictiveModelling.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msvm\u001b[39;00m \u001b[39mimport\u001b[39;00m SVC\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/isaac/Projects/TxADM_notebooks/docs/notebooks/Part2/03_MachineLearningNotes/02_PredictiveModelling.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m clf \u001b[39m=\u001b[39m SVC(C\u001b[39m=\u001b[39m\u001b[39m1.0\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/isaac/Projects/TxADM_notebooks/docs/notebooks/Part2/03_MachineLearningNotes/02_PredictiveModelling.ipynb#X11sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/my397/lib/python3.9/site-packages/sklearn/svm/_base.py:173\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    171\u001b[0m     check_consistent_length(X, y)\n\u001b[1;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 173\u001b[0m     X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[1;32m    174\u001b[0m         X,\n\u001b[1;32m    175\u001b[0m         y,\n\u001b[1;32m    176\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[1;32m    177\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    178\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    179\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m    180\u001b[0m     )\n\u001b[1;32m    182\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_targets(y)\n\u001b[1;32m    184\u001b[0m sample_weight \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(\n\u001b[1;32m    185\u001b[0m     [] \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m sample_weight, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat64\n\u001b[1;32m    186\u001b[0m )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/my397/lib/python3.9/site-packages/sklearn/base.py:596\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    594\u001b[0m         y \u001b[39m=\u001b[39m check_array(y, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_y_params)\n\u001b[1;32m    595\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 596\u001b[0m         X, y \u001b[39m=\u001b[39m check_X_y(X, y, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcheck_params)\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/my397/lib/python3.9/site-packages/sklearn/utils/validation.py:1074\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1069\u001b[0m         estimator_name \u001b[39m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1070\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1071\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mestimator_name\u001b[39m}\u001b[39;00m\u001b[39m requires y to be passed, but the target y is None\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1072\u001b[0m     )\n\u001b[0;32m-> 1074\u001b[0m X \u001b[39m=\u001b[39m check_array(\n\u001b[1;32m   1075\u001b[0m     X,\n\u001b[1;32m   1076\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49maccept_sparse,\n\u001b[1;32m   1077\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49maccept_large_sparse,\n\u001b[1;32m   1078\u001b[0m     dtype\u001b[39m=\u001b[39;49mdtype,\n\u001b[1;32m   1079\u001b[0m     order\u001b[39m=\u001b[39;49morder,\n\u001b[1;32m   1080\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   1081\u001b[0m     force_all_finite\u001b[39m=\u001b[39;49mforce_all_finite,\n\u001b[1;32m   1082\u001b[0m     ensure_2d\u001b[39m=\u001b[39;49mensure_2d,\n\u001b[1;32m   1083\u001b[0m     allow_nd\u001b[39m=\u001b[39;49mallow_nd,\n\u001b[1;32m   1084\u001b[0m     ensure_min_samples\u001b[39m=\u001b[39;49mensure_min_samples,\n\u001b[1;32m   1085\u001b[0m     ensure_min_features\u001b[39m=\u001b[39;49mensure_min_features,\n\u001b[1;32m   1086\u001b[0m     estimator\u001b[39m=\u001b[39;49mestimator,\n\u001b[1;32m   1087\u001b[0m     input_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mX\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m   1088\u001b[0m )\n\u001b[1;32m   1090\u001b[0m y \u001b[39m=\u001b[39m _check_y(y, multi_output\u001b[39m=\u001b[39mmulti_output, y_numeric\u001b[39m=\u001b[39my_numeric, estimator\u001b[39m=\u001b[39mestimator)\n\u001b[1;32m   1092\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/my397/lib/python3.9/site-packages/sklearn/utils/validation.py:856\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    854\u001b[0m         array \u001b[39m=\u001b[39m array\u001b[39m.\u001b[39mastype(dtype, casting\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39munsafe\u001b[39m\u001b[39m\"\u001b[39m, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    855\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 856\u001b[0m         array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(array, order\u001b[39m=\u001b[39;49morder, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[1;32m    857\u001b[0m \u001b[39mexcept\u001b[39;00m ComplexWarning \u001b[39mas\u001b[39;00m complex_warning:\n\u001b[1;32m    858\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    859\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mComplex data not supported\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(array)\n\u001b[1;32m    860\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mcomplex_warning\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.7/envs/my397/lib/python3.9/site-packages/pandas/core/generic.py:2064\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype: npt\u001b[39m.\u001b[39mDTypeLike \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m-> 2064\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_values, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'e'"
     ]
    }
   ],
   "source": [
    "# 5º Elección del algoritmo: \n",
    "# - un ejemplo Máquinas de Vectores de Soporte (SVM)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC(C=1.0, random_state=0)\n",
    "clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   class  cap-shape  cap-surface  cap-color  bruises  odor  gill-attachment  \\\n",
      "0      1          5            2          4        1     6                1   \n",
      "1      0          5            2          9        1     0                1   \n",
      "2      0          0            2          8        1     3                1   \n",
      "3      1          5            3          8        1     6                1   \n",
      "4      0          5            2          3        0     5                1   \n",
      "\n",
      "   gill-spacing  gill-size  gill-color  ...  stalk-surface-below-ring  \\\n",
      "0             0          1           4  ...                         2   \n",
      "1             0          0           4  ...                         2   \n",
      "2             0          0           5  ...                         2   \n",
      "3             0          1           5  ...                         2   \n",
      "4             1          0           4  ...                         2   \n",
      "\n",
      "   stalk-color-above-ring  stalk-color-below-ring  veil-type  veil-color  \\\n",
      "0                       7                       7          0           2   \n",
      "1                       7                       7          0           2   \n",
      "2                       7                       7          0           2   \n",
      "3                       7                       7          0           2   \n",
      "4                       7                       7          0           2   \n",
      "\n",
      "   ring-number  ring-type  spore-print-color  population  habitat  \n",
      "0            1          4                  2           3        5  \n",
      "1            1          4                  3           2        1  \n",
      "2            1          4                  3           2        3  \n",
      "3            1          4                  2           3        5  \n",
      "4            1          0                  3           0        1  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "# SVC tan solo funciona con variables continuas!\n",
    "# Ok, pero.... \n",
    "# pero podemos transformar variables categoricas a discrete integer values !\n",
    "# Ok, sigue siendo SVC.... !!!!!!!!!!!!\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "for i in df.columns:\n",
    "    df[i] = le.fit_transform(df[i])\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test,  y_train, y_test  = train_test_split(df.drop('population', axis=1),df['population'], test_size=0.2,random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(random_state=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "clf = SVC(C=1.0, random_state=0)\n",
    "clf.fit(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      1.00      0.64        65\n",
      "           1       0.93      0.66      0.77        62\n",
      "           2       0.17      0.01      0.02        93\n",
      "           3       0.46      0.44      0.45       237\n",
      "           4       0.69      0.83      0.76       827\n",
      "           5       0.42      0.27      0.33       341\n",
      "\n",
      "    accuracy                           0.61      1625\n",
      "   macro avg       0.52      0.54      0.49      1625\n",
      "weighted avg       0.57      0.61      0.58      1625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La evaluación de un modelo de entrenamiento\n",
    "\n",
    "La evaluación tiene como propósito dar validez y obtener confianza sobre el modelo propuesto.\n",
    "Dos perspectivas: Cualitativo y Cuantitativo.\n",
    "- Cualitativamente. ¿El modelo alcanza los objetivos de negocio, ayudando a la toma de decisiones? ¿Es posible integrarlo? ¿Se puede adaptar al stack tecnológico donde ha de aplicarse?, etc.\n",
    "- Cuantitativamente. Existen métricas sobre el rendimiento del consumo de recursos y el desempeño del modelo sobre el error de los resultados.\n",
    "\n",
    "### Métricas \n",
    "\n",
    "Las principales métricas depende del modelo. Generalmente, en regresión y clasificación las métricas son.\n",
    "\n",
    "En Clasificación:\n",
    "  \n",
    "<img src=\"images/TypeErrors.png\" width=\"60%\"/>\n",
    "\n",
    "> Source: NillsF blog\n",
    "\n",
    "- **Accuracy**: número de predicciones correctas como ratio de todas las predicciones hechas $ \\frac{TP+TN}{Total}.$\n",
    "- **Precision**: porcentaje de resultados positivos correctos sobre el total de resultados positivos  $\\frac{TP}{Results} = \\frac{TP}{TP+FP}$\n",
    "- **Recall**: $\\frac{TP}{Predictive Results} = \\frac{TP}{TP+FN}$\n",
    "- **f1-score**: es la media harmónica de la precisión y el recall: $\\frac{2}{recall^{-1}+precision^{-1}}$\n",
    "- **Area under curve (AUC)**: para problemas de clasificación binaria. La Receiver Operating Characteristic (ROC) curva de probabilidad y AUC representa el grado o medida de separación. Expresa el grado del modelo para distingir ambas clases.\n",
    "  \n",
    "- **Confusion matrix**: esta compuesta por la matriz de valores actuales x los valores de predicción . \n",
    "$$\\begin{equation}\n",
    "\\begin{pmatrix}\n",
    "TP & FN \\\\\n",
    "FP & TN \n",
    "\\end{pmatrix}\n",
    "\\end{equation}$$\n",
    "\n",
    "En Regresión:\n",
    "- **Mean absolute error (MAE)** es la suma de las diferencias absolutas entre las predicciones y el valor actual.\n",
    "- **Mean squared error (MSE)** representa la desviación estándard de las diferencias entre los valores de predicción y los valores observados.\n",
    "- **R squared ($R^2$)** es un indicador de la bondad del entreno de la predicción al actual valor.\n",
    "- **Adjusted R squared (Adj-$R^2$)** muestra como se ajusta la curva o linea de $R^2$  para los valores del modelo. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Éstas y muchas más métricas están implementadas en la librería de  Sci-kit\n",
    "- https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "- https://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "\n",
    "> Nota:\n",
    "> Iremos viendo estas métricas a lo largo de las unidades siguientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6092307692307692"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5725371139120128"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "metrics.precision_score(y_test,y_pred,average=\"weighted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall: 0.6092307692307692\n",
      "F1_score: 0.5759066983339758\n"
     ]
    }
   ],
   "source": [
    "print(\"Recall:\",metrics.recall_score(y_test,y_pred,average=\"weighted\"))\n",
    "print(\"F1_score:\",metrics.f1_score(y_test,y_pred,average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rendimiento Computacional\n",
    "\n",
    "\n",
    "Cada operación tiene un coste computacional. No sólo es tiempo sino memoria para almacenar las variables y valores intermedios.\n",
    "Las CPU/GPU tienen una capacidad computacional que suele medirse en Operaciones por Segundo. \n",
    "- MIPS\n",
    "- MFLOPS / GFLOPS\n",
    "\n",
    "Ese ratio influye en el tiempo de ejecución de los modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El lenguaje de programación y su compilador ayudan a obtener el mejor uso de la CPU/GPU. Son muchas las decisiones que se toman en la ejecución de un script. ¿Cuántos cores dispone una CPU/GPU para realizar la tarea? ¿Organización de las tareas y de las variables en memoría?, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Librerías como Tensorflow[https://www.tensorflow.org/?hl=es-419] tienen funcionalidades que explotan los recursos que nos ofrecen las GPU en la **computación paralela** de operaciones.\n",
    "\n",
    "```\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # Restrict TensorFlow to only allocate 1GB of memory on the first GPU\n",
    "  try:\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.LogicalDeviceConfiguration(memory_limit=1024)])\n",
    "    logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "En general, hay dos modelos de computación para optimizar la ejecución de análisis: el uso de **sistemas distribuidos** (i.e. Cloud Computing) y la **programación paralela**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En cualquier caso, la complejidad del modelo y la influencia de los datos (y su representación) tienen consecuencias sobre el rendimiento computacional y, por ende, en su aplicabilidad en un entorno real de explotación donde el tiempo de respuesta hacia el usuario sea razonable. Por ejemplo, un sistema de recomendación de productos que han de cargarse dinámicamente en una página web (i.e. Amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/0.15/modules/computational_performance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('my397')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8b38137d60f5ef6101ebd11fd805c6415d52a5c999d13278488bced8392256b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
